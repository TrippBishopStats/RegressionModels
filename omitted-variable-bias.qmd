---
title: "Omitted Variable Bias"
format: html
execute: 
  warning: false
  messages: false
---

```{r initial setup}
rm(list=ls()) # clean up the environment
# load libraries
library(MASS); library(tidyverse); library(broom); library(kableExtra)
library(latex2exp)
N <- 1000 # the number of observations in samples
base_theme <- theme_minimal() +
  theme(
    axis.title=element_text(size=14),
    axis.text=element_text(size=12)
  )
theme_set(base_theme) # set the theme for all plots
```

## The omitted variable problem
Leaving an important predictor out of a model impacts more than just the model
fit metrics. While the impact on $R^2$, AIC, or BIC is pretty straight-forward
to anticipate, there are other more subtle issues. When a model is misspecified
there is the potential for bias to be introduced into the slope parameter
estimates for the predictors that are in the model. How much bias, and in what
direction, depends upon the relationship between the omitted variable and the
model predictors and the response(s). A variable might be omitted because it
simply wasn't measured in the experiment or because its relationship to the
predictors and response(s) is not known to the researchers.

```{r}
matrix(
  c("Positive bias", "Negative bias", "Negative bias", "Positive bias"), 
  byrow=FALSE, ncol=2,
  dimnames=list(
    c("**$X_2$ & $Y$ are positively correlated**", 
      "**$X_2$ & $Y$ are negatively correlated**"),
    c("$X_1$ & $X_2$ are positively correlated", 
      "$X_1$ & $X_2$ are negatively correlated")
  )) |> kable(
    align="c",
    caption="Depending on the direction of the different correlations, the 
    nature of the bias of the slope parameter estimate of $X_1$ will be either 
    positive or negative")
```

## Exploring different relationships between the predictors and the response
We'll consider two predictors called $X_1$ and $X_2$ and a single response
variable $Y$. We will examine different relationships between the variables and
the parameter coefficient estimates that result under these scenarios.

### Generating the data
The examples will make use of the `mvrnorm` function from the `MASS` package.
This function is really useful for generating multivariate normal random samples
with control over the sample covariances and variances.

### Uncorrelated predictors
If $X_1$ and $X_2$ are not correlated, then the absence of $X_2$ from a model
with just $X_1$ and $Y$ will not bias the estimate for the slope parameter
associated with $X_1$.

Once we have generated the predictors, we can relate them to $Y$ with a linear
equation plus some noise. Recall that linear models assume that the error is
normally distributed with $\mu=0$ and constant variance $\sigma^2$.

```{r}
set.seed(42)
X <- mvrnorm(n=N,
             mu=c(0, 0),
             Sigma=matrix(c(3,0,0,3), ncol=2),
             empirical=TRUE)
Y <- 1.15*X[,1] + 0.4*X[,2] + rnorm(N, sd=1.25)
```

Before fitting a model and comparing the slope parameter estimates, let's take
a look at the correlations between the predictors and $X_2$ and $Y$ to confirm
that our simulated data is setup the way that we expect it to be.

:::{.callout-tip}
## Visualise and explore before modeling!
It's always a good idea to explore your data before fitting a model. This gives
you the opportunity to get to know the data and to uncover errors before 
starting the modeling process.
:::

```{r}
#| layout-ncol: 2
#| fig-cap:
#|    - "Scatter plot of $X_2$ vs $X_1$. The nearly flat LOESS line indicates that the two variables are uncorrelated."
#|    - "Scatter plot of $Y$ vs $X_2$. The sloped LOESS line indicates that the two variables are positively correlated."
r <- cor(X[,2], Y) |> round(digits=3)
tibble(
  x=X[,1],
  y=X[,2]
) |> 
  ggplot(aes(x,y)) + geom_point(alpha=0.4) +
  geom_smooth(method="loess", se=FALSE) +
  annotate("text", label=TeX("$r=0$"),x=3.5,
           y=-5,size=6,hjust=0,colour="magenta") +
  labs(
    x=TeX("$X_1$"),
    y=TeX("$X_2$")
  )
tibble(
  x=X[,2],
  y=Y
) |> 
  ggplot(aes(x,y)) + geom_point(alpha=0.4) +
  geom_smooth(method="loess", se=FALSE) +
  annotate("text",label=TeX(paste("$","r=",r,"$")),x=3.5,
           y=-5,size=6,hjust=0,colour="magenta") +
  labs(
    x=TeX("$X_2$"),
    y=TeX("$Y$")
  )
```

These data are behaving the way that we expect them to so we can continue on to
the task of fitting linear models.

First, let's fit a model with only $X_1$ and $Y$ and look at the slope parameter
estimate.

```{r}
df <- data.frame(X,Y)
lm(Y ~ X1, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```
Then, if we include $X_2$ in the model, notice that the coefficient estimate for
the slope parameter for $X_1$ does not change.

```{r}
lm(Y ~ X1 + X2, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

### Positive, moderate ($r=0.5$) correlation between predictors
Now, the predictors are moderately correlated ($r=0.5$). Here, $X_1$
and $X_2$ are positively correlated and $X_2$ and $Y$ are also positively
correlated. This means that when $X_2$ is omitted from the model that contains
both $X_1$ and $Y$, the slope parameter estimate for $X_1$ will be positively
biased.

```{r}
set.seed(42)
X <- mvrnorm(n=N,
             mu=c(0, 0),
             Sigma=matrix(c(3,1.5,1.5,3), ncol=2),
             empirical=TRUE)
Y <- 1.15*X[,1] + 0.4*X[,2] + rnorm(N, sd=1.25)
df <- data.frame(X,Y)
```

Notice that the coefficient estimate for X2 has increased.
```{r}
lm(Y ~ X1, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

Both predictors are now in the model and notice that the slope parameter 
estimate for $X_1$ is now much close to the true value.
```{r}
lm(Y ~ X1 + X2, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

### Negative correlation between between $X_2$ and $Y$
The predictors are still moderately correlated with $r=0.5$, but $X_2$ and $Y$
are now negatively correlated. This means that when $X_2$ is omitted from the
model that contains both $X_1$ and $Y$, the slope parameter estimate for $X_1$
will be negatively biased.

```{r}
set.seed(42)
X <- mvrnorm(n=N,
             mu=c(0, 0),
             Sigma=matrix(c(3,1.5,1.5,3), ncol=2),
             empirical=TRUE)
Y <- 1.15*X[,1] - 0.4*X[,2] + rnorm(N, sd=1.25)
df <- data.frame(X,Y)
```

Notice now that the coefficient estimate for X1 has decreased compared to what
we know to be the "truth" based on the data we simulated.
```{r}
lm(Y ~ X1, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

Putting both predictors in the model observe how slope parameter estimate of 
$X_1$ is now much closer to what we expect it to be.
```{r}
lm(Y ~ X1 + X2, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

### Positive correlation between predictors but $X_2$ is uncorrelated with $Y$
The omitted predictor can be very strongly correlated with predictors in the
model, but if it is not correlated with the response variable, then its omission
will not have any impact on the model. The following code demonstrates this.

```{r}
set.seed(42)
X <- mvrnorm(n=N,
             mu=c(0, 0),
             Sigma=matrix(c(3,1.5,1.5,3), ncol=2),
             empirical=TRUE)
Y <- 1.15*X[,1] + rnorm(N, sd=1.25)
df <- data.frame(X,Y)
```

The slope parameter estimate in the simple linear regression model is right 
about what we expect it to be.
```{r}
lm(Y ~ X1, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

When $X_2$ is introduced as a second predictor, the slope parameter estimate for
$X_1$ hardly changes.
```{r}
lm(Y ~ X1 + X2, data=df) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

## Characterising the bias - Monte Carlo simulations
We can get a sense for just how variable the bias is by perform a Monte Carlo
simulation with 1000 replications of the experiment. The idea here is that we
perform the simulation many times, recording the slope parameter coefficient
estimate and its standard error for each run of the simulation. That lets us
build up an empirical distribution of the slope parameter coefficient
estimate which we can use to ask questions about the nature of the bias 
introduced by omitting a key variable.

In this scenario, we will use the set up where $X_1$ and $X_2$ are positively 
correlated, but $X_2$ is negatively correlated with $Y$. In this case, we expect
that the slope parameter coefficient estimate for $X_1$ will be negatively 
biased.

```{r}
#| label: fig-cap
#| fig-cap: "Empirical distributions of the coefficient estimate and its standard error. Green line represent the 95% CIs."
#| fig-subcap: 
#|   - "Estimates of the slope parameter coefficient"
#|   - "The standard error of the coefficient estimate."
#| layout-ncol: 2

n <- 1000 # the number of simulations to run
# store the model output for each trial in a pair of vectors
estimates <- stderrs <- numeric(length=n)
capture <- logical(length=n)
t_star <- qt(0.975, df=(N-2))

for(i in 1:n) {
  X <- mvrnorm(n=N,
             mu=c(0, 0),
             Sigma=matrix(c(3,1.5,1.5,3), ncol=2),
             empirical=TRUE)
  Y <- 1.15*X[,1] - 0.4*X[,2] + rnorm(N, sd=1.25)
  df <- data.frame(X,Y)
  mod.sum <- lm(Y ~ X1, data=df) |> summary()
  
  est <- mod.sum$coefficients[2,1]
  stderr <- mod.sum$coefficients[2,2]
  estimates[i] <- est
  stderrs[i] <- stderr
  
  CI_lower <- est - t_star*stderr
  CI_upper <- est + t_star*stderr
  
  # does the 95% CI for this model run actually capture the true parameter
  # coefficient?
  capture[i] <- (CI_lower < 1.15 & CI_upper > 1.15)
}
CI_est_95 <- quantile(estimates, probs=c(0.025, 0.975))
CI_err_95 <- quantile(stderrs, probs=c(0.025, 0.975))
df <- tibble(coeff = estimates, stderr = stderrs)
df |> 
  ggplot(aes(x=coeff)) + 
    geom_histogram(colour="white", fill="steelblue", bins=20, alpha=0.6) +
    geom_vline(xintercept=CI_est_95[1], colour="seagreen", linewidth=1) +
    geom_vline(xintercept=CI_est_95[2], colour="seagreen", linewidth=1) +
    labs(x="Coefficient estimates",y="Frequency")
df |> 
  ggplot(aes(x=stderr)) + 
    geom_histogram(colour="white", fill="steelblue", bins=20, alpha=0.6) +
    geom_vline(xintercept=CI_err_95[1], colour="seagreen", linewidth=1) +
    geom_vline(xintercept=CI_err_95[2], colour="seagreen", linewidth=1) +
    labs(x="Std. error",y="Frequency")
```

Temp playground:
```{r}
X <- mvrnorm(n=N,
             mu=c(0, 0),
             Sigma=matrix(c(3,1.5,1.5,3), ncol=2),
             empirical=TRUE)
Y <- 1.15*X[,1] - 0.4*X[,2] + rnorm(N, sd=1.25)
df <- data.frame(X,Y)
mod.sum <- lm(Y ~ X1, data=df) |> summary()

est <- mod.sum$coefficients[2,1]
stderr <- mod.sum$coefficients[2,2]
CI_lower <- est - t_star*stderr
CI_upper <- est + t_star*stderr

# does the 95% CI for this model run actually capture the true parameter
# coefficient?
capture <- (CI_lower < 1.15 & CI_upper > 1.15)
```

## Kicking the tyres with `mtcars`
Now lets try with the mtcars dataset rather than simulated data. Use `hp` and 
`wt` as predictors and `mpg` as the response. Try `hp` as the omitted variable.

```{r}
cor(mtcars$hp, mtcars$wt)
cor(mtcars$hp, mtcars$mpg)
```

`hp` and `wt` are pretty strongly positively correlated and `hp` and `mpg` are
strongly negatively correlated. Omitting `hp` from the model, we can see that
there is a negative bias in the slope parameter estimate for `wt` when `wt` is 
the only predictor.

```{r}
lm(mpg ~ wt, data=mtcars) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
lm(mpg ~ wt + hp, data=mtcars) |> summary() |> tidy() |> 
  mutate(
    p.value = scales::pvalue(p.value, accuracy=0.0001)
  ) |> kable(digits=4, align=c("l",rep("r", times=4)))
```

