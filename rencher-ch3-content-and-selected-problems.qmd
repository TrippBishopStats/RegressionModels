---
title: "Characterising Multivariate Data"
format: html
execute: 
  warning: false
  messages: false
---

```{r}
rm(list=ls())
library(tidyverse)
library(matlib)
library(kableExtra)
```


## Univariate mean and variance

### The sample mean

$$
\bar{y} = \frac{\sum_{i=1}^n y_i}{n}
$$
The sample mean is an unbiased and consistent estimator of the population mean, 
denoted $\mu$. In other words, $E[\bar{y}]=\mu$ and 
$\text{var}(\bar{y})\to 0\text{ as } n\to\infty$.

### The sample variance

$$
s^2 = \frac{\sum_{i=1}^n(y_i-\bar{y})^2}{n-1}
$$
It can be shown that $s^2$ is an unbiased and consistent estimator of the
population variance, $\sigma^2$. In other words, $E[s^2]=\sigma^2$ and 
$\text{var}(s^2)\to 0\text{ as } n\to\infty$.

## Covariance

```{r}
# height and weight data for college males
h <- c(69,74,68,70,72,67,66,70,76,68,72,79,74,67,66,71,74,75,75,76)
w <- c(153,175,155,135,172,150,115,137,200,130,140,265,185,112,140,150,165,185,210,220)

males <- cbind(h,w)
n <- nrow(males)
```

$$
S_{xy} = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{n-1}
$$

We can use matrix algebra to determine the covariance matrix of the college
males data, where height and weight form a bivariate random variable. We first
have to centre the data. There are a lot of ways that the data can be centred.
We can use the `apply` function applied across the columns with the `mean`
function to determine the column means. Then, these values are substracted from
the value in each cell by using a matrix difference. Another way to do this,
which is pretty cool is with a $\pmb{j}$ vector and then using a series of
matrix multiplications involving the $\pmb{j}$ vector and its transpose to
create a $20\times 2$ matrix of means and then doing the same matrix difference
to create the scaled values.

$$
\pmb{S}_m = \pmb{M} - \frac{1}{n}\pmb{j}'\pmb{j}\pmb{M}
$$
where $\pmb{M}$ is the matrix of original college males data and $\pmb{S}_m$ is 
the scaled college male data.

```{r}
#scaling "by hand"

# using the crutch of apply
scaled_males <- males - matrix(rep(apply(males, 2, mean), each=n), ncol=2)

# using a "j" vector
j <- matrix(rep(1, times=n), ncol=1)
scaled_males <- males - 1/n*j%*%t(j)%*%males # this second matrix is 20x2 matrix of means
```

The last way is super easy. Using the `scale` function from base R, the centring
(and scaling if desired) can be done is a very simple line of code.


```{r}
scaled_males <- scale(males, center=TRUE, scale=FALSE)
```

We can then create the correlation matrix with the following matrix operation:

$$
S_{hw} = \frac{1}{n-1}S_{m}'S_{m}
$$

```{r}
Shw <- t(scaled_males)%*%scaled_males/(n-1)
```

We can check our work with the `cov` function from the `stats` package.

```{r}
all.equal(Shw, cov(males))
```

This shows that the two approaches are equivalent, so the code above did what we
wanted it to do.

## Correlation
Covariance is a really important concept in multivariate statistics, but the
covariance between two variables isn't necessarily easy to interpret because the
scales of the individual variables has a very strong influence on the value that
is calculated. A scaled value is much more useful to determine the degree to
which two random variables covary.

We can scale the covariance by dividing by $s_x$ and $s_y$.
$$
r = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^n(x_i-\bar{x})^2 \sum_{i=1}^n(y_i-\bar{y})^2}}
$$

### Wild geometric explanation of correlation


$$
\pmb{x}\cdot\pmb{y} = \cos\theta||\pmb{x}||\cdot||\pmb{y}||
$$
where $||\pmb{x}|| = \sqrt{\pmb{x}'\pmb{x}}$



$$
\cos\theta = \frac{(\pmb{x} - \bar{x}\pmb{j})'(\pmb{y} - \bar{y}\pmb{j})}{\sqrt{(\pmb{x} - \bar{x}\pmb{j})'(\pmb{x} - \bar{x}\pmb{j})\cdot(\pmb{y} - \bar{y}\pmb{j})(\pmb{y} - \bar{y}\pmb{j})}} = r_{xy}
$$

```{r}
X <- males[,1,drop=FALSE]
Y <- males[,2,drop=FALSE]

x_bar <- mean(X)
y_bar <- mean(Y)

XY <- t(X - x_bar*j)%*%(Y - y_bar*j)
X2 <- t(X - x_bar*j)%*%(X - x_bar*j)
Y2 <- t(Y - y_bar*j)%*%(Y - y_bar*j)

R <- XY/sqrt(X2*Y2) |> as.numeric()
```

```{r}
all.equal(R, cor(X,Y))
```



## Mean vectors

```{r}
# Calcium in soil data
y1 <- c(35,35,40,10,6,20,35,35,35,30)
y2 <- c(3.5,4.9,30.0,2.8,2.7,2.8,4.6,10.9,8.0,1.6)
y3 <- c(2.80,2.70,4.38,3.21,2.73,2.81,2.88,2.90,3.28,3.20)

soil <- cbind(y1,y2,y3)
n <- nrow(soil)
# calculate column sums

j <- matrix(rep(1,times=n), ncol=1)

row_means <- 1/n*t(j)%*%soil
```




## Covariance matrices


$$
\pmb{S} = \frac{\pmb{Y}'(\pmb{I} - \frac{1}{n}\pmb{J})\pmb{Y}}{n-1}
$$

```{r}
Y <- soil
J <- j%*%t(j)
I <- diag(1, ncol=n, nrow=n)
S <- t(Y)%*%(I - 1/n*J)%*%Y/(n-1)
all.equal(S, cov(Y))
```

$$
\pmb{S} = \frac{\sum_{i=1}^n (\pmb{y}_i-\pmb{\bar{y}})'(\pmb{y}_i-\pmb{\bar{y}})}{n-1}
$$
which can also be written as
$$
\pmb{S} = \frac{\sum_{i=1}^n \pmb{y}_i\pmb{y}_i' - n\pmb{\bar{y}}\pmb{\bar{y}}'}{n-1}
$$

## Correlation matrices
The correlation matrix is very easy to create once we have the covariance
matrix. One really easy way to create $\pmb{R}$, the correlation matrix, is to
"divide" the covariance matrix, $\pmb{S}$, by the standard deviations of the
variables. We can create a diagonal matrix, $\pmb{D}$, that uses the diagonal of
$\pmb{S}$ as a standing point. The diagonal of $\pmb{S}$ is the variances of the
variables, so we need to take the square root. Then, we need to divide the
values of $\pmb{S}$ by all of the standard deviations of the variables.

$$
\pmb{R} = \pmb{D}_s^{-1}\pmb{S}\pmb{D}_s^{-1}
$$

```{r}
D <- diag(sqrt(diag(S)), ncol=ncol(S))
# for fun, we can invert without the solve function
D_inv <- diag(1/sqrt(diag(S)), ncol=ncol(S)) 
# the proof
all.equal(D_inv,solve(D))

R <- D_inv%*%S%*%D_inv

# proof that they're equal. Have to unname the matrix produced by cor(Y) to get
# all.equal to work as expected.
all.equal(R, unname(cor(Y)))
```

```{r}
ltx_Dinv <- latexMatrix(D_inv, matrix="pmatrix")$matrix
ltx_S <- latexMatrix(S, matrix="pmatrix")$matrix
ltx_DinvprodS <- latexMatrix(D_inv%*%S, matrix="pmatrix")$matrix
ltx_R <- latexMatrix(R, matrix="pmatrix")$matrix
```

$`r ltx_Dinv` `r ltx_S` = `r ltx_DinvprodS`$

This first step divides each element in $\pmb{S}$ by one of the variable
standard deviations. For example, $s_{11}$ is divided by the standard deviation
of $y_1$, and $s_{12}$ by the standard deviation of $y_2$, etc.

$`r ltx_DinvprodS` `r ltx_Dinv` = `r ltx_R`$

Then, to bring things home, we have to then divide each element by the other
standard deviation represented in the particular covariance. For example,
element $s_{13}$ represents the covariance of $y_1$ and $y_3$. To compute the
correlation, we need $s_{13}/s_1s_3$, and by right multiplying by
$\pmb{D}^{-1}$, we are achieving this.


and conversely,

$$
\pmb{S} = \pmb{D}_s\pmb{R}\pmb{D}_s
$$

```{r}
S <- D%*%R%*%D
all.equal(S, unname(cov(Y)))
```


## Linear combinations of variables


```{r}
y1 <- c(51,27,37,42,27,43,41,38,36,26,29)
y2 <- c(36,20,22,36,18,32,22,21,23,31,20)
y3 <- c(50,26,41,32,33,43,36,31,27,31,25)
y4 <- c(35,17,37,34,14,35,25,20,25,32,26)
y5 <- c(42,27,30,27,29,40,38,16,28,36,25)

Y <- cbind(y1,y2,y3,y4,y5) |> unname()
n <- nrow(Y)
p <- ncol(Y)

a <- matrix(sample(-9:9, size=p, replace=TRUE))
b <- matrix(sample(-9:9, size=p, replace=TRUE))
j <- matrix(rep(1, times=n), ncol=1)
J <- j%*%t(j)
I <- diag(ncol=n, nrow=n)

# get column means
y_bar <- t(t(j)%*%Y/n)

# create covariance matrix
S <- (t(Y)%*%(I-1/n*J)%*%Y)/(n-1)

a <- matrix(c(3,-2,4,-1,1))
b <- matrix(c(1,3,-1,1,-2))

z <- Y%*%a
w <- Y%*%b

# mean and variance of RV z
z_bar <- t(a)%*%y_bar
s2_z <- t(a)%*%S%*%a

# mean and variance of RV z
w_bar <- t(b)%*%y_bar
s2_w <- t(b)%*%S%*%b

# the sample covariance between w & z
s_wz <- t(a)%*%S%*%b

r_wz <- s_wz/sqrt(t(a)%*%S%*%a * t(b)%*%S%*%b)
```


```{r}
#| echo: false

ltx_Y <- latexMatrix(Y, matrix="pmatrix")$matrix
ltx_z <- latexMatrix(z, matrix="pmatrix")$matrix
ltx_zt <- latexMatrix(t(z), matrix="pmatrix")$matrix
ltx_w <- latexMatrix(w, matrix="pmatrix")$matrix
ltx_wt <- latexMatrix(t(w), matrix="pmatrix")$matrix
ltx_at <- latexMatrix(t(a), matrix="pmatrix")$matrix
ltx_ybar <- latexMatrix(y_bar, matrix="pmatrix")$matrix
ltx_a <- latexMatrix(a, matrix="pmatrix")$matrix
ltx_at <- latexMatrix(t(a), matrix="pmatrix")$matrix
ltx_b <- latexMatrix(b, matrix="pmatrix")$matrix
ltx_bt <- latexMatrix(t(b), matrix="pmatrix")$matrix
ltx_S <- latexMatrix(S, matrix="pmatrix")$matrix
ltx_aS <- latexMatrix(t(a)%*%S, matrix="pmatrix")$matrix
ltx_aSa <- latexMatrix(t(a)%*%S%*%a, matrix="pmatrix")$matrix
```

### The data
The data consist of 5 variables with 11 subjects.

```{r}
#| echo: false
as.data.frame(Y) |> 
  kable(
    col.names=c("$y_1$","$y_2$","$y_3$","$y_4$","$y_5$"),
    caption="Reponse times for five probe word positions."
  )
```

The mean of each variable is given by $\pmb{\bar{y}} = \pmb{j}'\pmb{Y}/n$, 
$\pmb{\bar{y}}=`r round(y_bar,2)`$.

The covariance matrix for the multivariate distribution of the 5 random variables is
$\pmb{S} = \pmb{Y}'(\pmb{I}-\frac{1}{n}\pmb{J})\pmb{Y}$ and for this data,

$\pmb{S} = `r ltx_S`$.

Now, we will define $\pmb{z}$ to be a linear combination of $y_1$, $y_2$, $y_3$,
$y_4$ and $y_5$ with the constants being contained in the vector 

$\pmb{a}' = `r ltx_at`$. 

$\pmb{z}$ is the matrix product of $\pmb{a}$ and the columns of $\pmb{Y}$ such 
that $\pmb{z} = \pmb{Y}\pmb{a}$.

$\pmb{z}' = `r ltx_zt`$

Now, we will define $\pmb{w}$ to be a linear combination of $y_1$, $y_2$, $y_3$,
$y_4$ and $y_5$ with the constants being contained in the vector 

$\pmb{b}' = `r ltx_bt`$.

$\pmb{w}$ is the matrix product of $\pmb{b}$ and the columns of $\pmb{Y}$ such 
that $\pmb{w} = \pmb{Y}\pmb{b}$.

$\pmb{w}' = `r ltx_wt`$

### The mean of $z$
Since $\pmb{z}$ is a linear combination of the random variables $Y_1$ - $Y_5$,
it has both a mean and a variance.

The mean of $\pmb{z}$ is simply the weighted means of the $Y_i$, where the
weights are the elements of $\pmb{a}$.

$$
\bar{z} = \pmb{a}'\pmb{\bar{y}}
$$

In this example,

$\bar{z} = `r ltx_at` `r ltx_ybar` = `r z_bar`$

### The variance of $\pmb{z}$
We can also calculate the variance of $\pmb{z}$ using the covariance matrix of
$Y$. This variance can be expressed as a quadratic form 
$s^2_z=\pmb{a}'\pmb{S}\pmb{a}$, which is a scalar quantity because we have a 
single random function, $\pmb{z}$ we're considering. We'll see in a minute how
we can extend this when we have multiple random functions that we're examining
simultaneously.

$\pmb{a'S}$

$`r ltx_at` `r ltx_S` = `r ltx_aS`$

$\pmb{a'Sa}$

$`r ltx_aS` `r ltx_a` = `r ltx_aSa`$

### The random variable $W$

$W$ is another random variable that is a linear combination of the columns of 
$Y$. It's mean and variance are computed in exactly the same way as with $Z$.

### The covariance of $Z$ and $W$
We can constructs a bivariate distribution of $Z$ and $W$. We already have the
individual variances of these two random variables and the covariance of them
is surprisingly easy to compute.

$s_{wz} = \pmb{a}'\pmb{Sb}$, 

$`r ltx_at` `r ltx_S` `r ltx_b` = `r s_wz`$





and using this we can also quite easily compute the
correlation between $W$ and $Z$ as

$$
r_{wz} = \frac{s_{wz}}{\sqrt{s^2_zs^2_w}} = \frac{\pmb{a}'\pmb{Sb}}{\sqrt{\pmb{a}'\pmb{Sa}\cdot\pmb{b}'\pmb{Sb}}}
$$

$$
r_{wz} = \frac{`r s_wz`}{`r round(sqrt(s2_z * s2_w), 2)`} = `r round(r_wz, 4)`
$$

## Multiple linear combinations
If we have the situation were we have multiple linear combinations of a vector of data $\pmb{y}$, then we can describe this mathematically with the following equations:
$$
\begin{aligned}
&z_1 = a_{11}y_1+a_{12}y_2+\cdots+a_{1p}y_p = &\pmb{a}_1'\pmb{y}\\
&z_2 = a_{21}y_1+a_{22}y_2+\cdots+a_{2p}y_p = &\pmb{a}_2'\pmb{y}\\
&\hspace{0.25cm}\vdots &\vdots\hspace{0.25cm} \\
&z_k = a_{k1}y_1+a_{k2}y_2+\cdots+a_{kp}y_p = &\pmb{a}_k'\pmb{y}\\
\end{aligned}
$$
We can, of course, represent this as a matrix equation:
$$
\pmb{z}=
\begin{pmatrix}
z_1 \\
z_2 \\
\vdots\\
z_k \\
\end{pmatrix} = 
\begin{pmatrix}
\pmb{a}_1'\pmb{y}\\
\pmb{a}_2'\pmb{y}\\
\vdots\\
\pmb{a}_k'\pmb{y}
\end{pmatrix} = 
\begin{pmatrix}
\pmb{a}_1'\\
\pmb{a}_2'\\
\vdots\\
\pmb{a}_k'
\end{pmatrix}\pmb{y}=
\pmb{A}\pmb{y}
$$
Now we're representing the $k$ linear combinations of $\pmb{y}$ as a vector.
$\pmb{A}$ now represents the coefficients of all of the linear transformations
in a compact way.

If we have many $\pmb{y}_i$ observations of data, for example, then we can think
of each $\pmb{z}$ as a random draw. We can then determine descriptive statistics
like the sample mean and covariance of $\pmb{z}$.

### Sample mean of $\pmb{z}$
This is quite straight-forward. We just need to first compute the means of the
variables $\pmb{y}$ as we've seen above. Then, the sample mean of $\pmb{z}$ is
given by
$$
\pmb{\bar{z}} = \begin{pmatrix}
\pmb{a}_1'\pmb{\bar{y}}\\
\pmb{a}_2'\pmb{\bar{y}}\\
\vdots\\
\pmb{a}_k'\pmb{\bar{y}}
\end{pmatrix} = 
\begin{pmatrix}
\pmb{a}_1'\\
\pmb{a}_2'\\
\vdots\\
\pmb{a}_k'
\end{pmatrix}\pmb{\bar{y}} = \pmb{A}\pmb{\bar{y}}
$$

where each element of $\pmb{z}$ is a weighted sum of the individual variables 
$\pmb{y}_i$, just like we did above. Now, we have $k$ of them. One for each of
the $k$ linear combinations.

### The sample covariance matrix of the $\pmb{z}$'s


$$
\begin{aligned}
S_z &=  \begin{pmatrix}
\pmb{a}_1'\pmb{S}\pmb{a}_1 & \pmb{a}_1'\pmb{S}\pmb{a}_2 & \cdots & \pmb{a}_1'\pmb{S}\pmb{a}_k\\
\pmb{a}_2'\pmb{S}\pmb{a}_1 & \pmb{a}_2'\pmb{S}\pmb{a}_2 & \cdots & \pmb{a}_2'\pmb{S}\pmb{a}_k\\
\vdots&\vdots&&\vdots\\
\pmb{a}_k'\pmb{S}\pmb{a}_1 & \pmb{a}_k'\pmb{S}\pmb{a}_2 & \cdots & \pmb{a}_k'\pmb{S}\pmb{a}_k\\
\end{pmatrix}\\[0.5cm]
&= \begin{pmatrix}
\pmb{a}_1'(\pmb{S}\pmb{a}_1 & \pmb{S}\pmb{a}_2 & \cdots & \pmb{S}\pmb{a}_k)\\
\pmb{a}_2'(\pmb{S}\pmb{a}_1 & \pmb{S}\pmb{a}_2 & \cdots & '\pmb{S}\pmb{a}_k)\\
\vdots&\vdots&&\vdots\\
\pmb{a}_k'(\pmb{S}\pmb{a}_1 & \pmb{S}\pmb{a}_2 & \cdots & \pmb{S}\pmb{a}_k)\\
\end{pmatrix}\\[0.5cm]
&=\begin{pmatrix}
\pmb{a}_1'\\
\pmb{a}_2'\\
\vdots\\
\pmb{a}_k'
\end{pmatrix}
\begin{pmatrix}
\pmb{S}\pmb{a}_1 & \pmb{S}\pmb{a}_2 & \cdots & \pmb{S}\pmb{a}_k
\end{pmatrix}\\[0.5cm]
&=\begin{pmatrix}
\pmb{a}_1'\\
\pmb{a}_2'\\
\vdots\\
\pmb{a}_k'
\end{pmatrix}
\pmb{S}
\begin{pmatrix}
\pmb{a}_1 & \pmb{a}_2 & \cdots & \pmb{a}_k\\
\end{pmatrix}\\[0.5cm]
&= \pmb{A}\pmb{S}\pmb{A}'
\end{aligned}
$$
Recall that $\pmb{A}$ is defined as a vector whose elements are the row vectors
$\pmb{a}_i$ and so the way that $\pmb{A}$ and $\pmb{A}'$ are denoted above may 
seem backwards.

Notice now that the trace of $\pmb{A}\pmb{S}\pmb{A}'$, 
$tr(\pmb{A}'\pmb{S}\pmb{A})$ is simply the sum of the quadratic forms on the 
diagonal of $\pmb{S}$.

$$
tr(\pmb{A}\pmb{S}\pmb{A}') = \sum_{i=1}^k \pmb{a}_i'\pmb{S}\pmb{a}_i
$$

## The theory in practice
Now, we can apply these ideas to the data defined above. We first need to define
the linear functions:
$$
\begin{aligned}
&z_1 = y_1+y_2+y_3+y_4+y_5\\
&z_2 = 2y_1-3y_2+y_3-2y_4-y_5\\
&z_3 = -y_1-2y_2+y_3-2y_4+3y_5
\end{aligned}
$$
In matrix form, this is
$$
\begin{pmatrix}
z_1\\
z_2\\z_3
\end{pmatrix} = 
\begin{pmatrix}
1 & 1 & 1 & 1 & 1\\
2 & -3 & 1 & -2 & -1\\
-1 & -2 & 1 & -2 & 3
\end{pmatrix}
\begin{pmatrix}
y_1\\
y_2\\
y_3\\
y_4\\
y_5
\end{pmatrix}
$$

More succinctly, this is simply $\pmb{z} = \pmb{A}\pmb{y}$.

```{r}
A <- matrix(c(1,1,1,1,1,
              2,-3,1,-2,-1,
              -1,-2,1,-2,3), byrow=TRUE, ncol=5, nrow=3)

z_bar <- A%*%y_bar 
S_z <- A%*%S%*%t(A)
ltx_z <- latexMatrix(z_bar, matrix="pmatrix")$matrix
ltx_Sz <- latexMatrix(S_z, matrix="pmatrix")$matrix
```

$\pmb{\bar{z}}=\pmb{A}\pmb{\bar{y}} = `r ltx_z`$.

We can now compute the covariance matrix for $z$ as

$\pmb{S} = \pmb{ASA}' = `r ltx_Sz`$.

Now that we have computed the covariance matrix, we can generate the
correlation matrix using the equation $\pmb{R} = \pmb{D}^{-1}\pmb{SD}^{-1}$.

```{r}
D_z <- diag(sqrt(diag(S_z)), ncol=ncol(S_z))
D_z_inv <- solve(D_z)
R_z <- D_z_inv%*%S_z%*%D_z_inv 
ltx_D_z <- latexMatrix(D_z, matrix="pmatrix")$matrix
ltx_R_z <- latexMatrix(R_z, matrix="pmatrix")$matrix
```

$\pmb{R} = \pmb{D}^{-1}\pmb{SD}^{-1} = `r ltx_R_z`$.

The diagonal matrix computed from $\pmb{S}_z$ is

$\pmb{D}_z = `r ltx_D_z`$.

## Distance between vectors - Mahalanobis distance
If we have 2 vectors in some vector space, we can measure the Euclidean square 
of the distance between them using the following equation:

$$
d^2 = (\pmb{y}_1 - \pmb{y}_2)'(\pmb{y}_1 - \pmb{y}_2)
$$
When we want to measure the "statistical" distance between two observations,
however, we need to take covarainces into account. Mahalanobis proposed the 
following in 1936.

$$
d^2 = (\pmb{y}_1 - \pmb{y}_2)'\pmb{S}^{-1}(\pmb{y}_1 - \pmb{y}_2)
$$
This gives the square of the distance between two vectors (observations) in
$\mathbb{R}^p$ where there are $p$ variables associated with each observation.

```{r}
y1 <- t(Y[1,,drop=FALSE])
y2 <- t(Y[2,,drop=FALSE])
d2 <- t(y1 - y2)%*%solve(S)%*%(y1-y2) |> as.numeric()

mahal_dist <- mahalanobis(Y[1,,drop=FALSE], center=Y[2,,drop=FALSE], S)
all.equal(d2, mahal_dist)
```
If we just compare two observations in the $\mathbb{R}^5$ space, the first and
second rows of $\pmb{Y}$, we can see that their Mahalanobis distance is 
$d^2=`r d2`$. This agrees precisely with the output of the `mahalanobis` 
function from the `stats` package.

::: {.callout-note}
## Note
It is important to note here that the $\pmb{y_i}$ vectors are observations 
rather than variables. So here each $\pmb{y}_i$ is a vector in $\mathbb{R}^5$
rather than $\mathbb{R}^{11}$. The notation used in Rencher is a little 
confusing in this very last section of chapter 3.
:::

## Selected exercises

### Problem 10

```{r}
y1 <- c(35,35,40,10,6,20,35,35,35,30)
y2 <- c(3.5,4.9,30.0,2.8,2.7,2.8,4.6,10.9,8.0,1.6)
y3 <- c(2.80,2.70,4.38,3.21,2.73,2.81,2.88,2.90,3.28,3.20)

Y <- cbind(y1,y2,y3)
```

a.) Calculate $\pmb{S}$ from the data matrix $\pmb{Y}$ using the equation

$$
\pmb{S} = \frac{1}{n-1}\pmb{Y}'\bigg(\pmb{I}-\frac{1}{n}\pmb{J}\bigg)\pmb{Y}
$$
```{r}
n <- nrow(Y)
I <- diag(ncol=n, nrow=n)
J <- matrix(rep(1, times=n^2), ncol=n)
S <- t(Y)%*%(I - J/n)%*%Y/(n-1)

all.equal(S, cov(Y))
ltx_S <- latexMatrix(round(S, 2), matrix="pmatrix")$matrix
```
$\pmb{S} = `r ltx_S`$

b.) Obtain $\pmb{R}$ by calculating $r_{12}$, $r_{13}$, and $r_{23}$.

```{r}
r_12 <- S[1,2]/sqrt(S[1,1]*S[2,2])
r_13 <- S[1,3]/sqrt(S[1,1]*S[3,3])
r_23 <- S[3,2]/sqrt(S[3,3]*S[2,2])

R_manual <- diag(ncol=3,nrow=3)
R_manual[1,2] <- R_manual[2,1] <- r_12
R_manual[1,3] <- R_manual[3,1] <- r_13
R_manual[2,3] <- R_manual[3,2] <- r_23
```


$$
\begin{aligned}
r_{12} &= \frac{s_{12}}{\sqrt{s^2_1s^2_2}} = `r r_12`\\
r_{13} &= \frac{s_{12}}{\sqrt{s^2_1s^2_2}} = `r r_13`\\
r_{23} &= \frac{s_{12}}{\sqrt{s^2_1s^2_2}} = `r r_23`\\[0.25cm]
\pmb{R} &= \begin{pmatrix}
1 & `r r_12` & `r r_13`\\
`r r_12` & 1 & `r r_23`\\
`r r_13` & `r r_23` & 1\\
\end{pmatrix}
\end{aligned}
$$
c.) Find $\pmb{R}$ using $\pmb{R}=\pmb{D}^{-1}\pmb{S}\pmb{D}^{-1}$.

```{r}
D <- diag(sqrt(diag(S)), ncol=ncol(S))
D_inv <- solve(D)

R <- D_inv%*%S%*%D_inv
all.equal(R, unname(cor(Y)))
all.equal(R, R_manual)
ltx_R <- latexMatrix(R, matrix="pmatrix")$matrix
```

$\pmb{R} = `r ltx_R`$.

### Problem 11

```{r}
y1 <- c(35,35,40,10,6,20,35,35,35,30)
y2 <- c(3.5,4.9,30.0,2.8,2.7,2.8,4.6,10.9,8.0,1.6)
y3 <- c(2.80,2.70,4.38,3.21,2.73,2.81,2.88,2.90,3.28,3.20)

Y <- cbind(y1,y2,y3)
n <- nrow(Y)
I <- diag(ncol=n, nrow=n)
J <- matrix(rep(1, times=n^2), ncol=n)
S <- t(Y)%*%(I-J/n)%*%Y/(n-1)
```


a.) Find the generalised sample variance of the calcium data $|\pmb{S}|$ as
in (3.77).

```{r}
S_eigs <- eigen(S, only.values=TRUE)
all.equal(det(S), prod(S_eigs$values))
```
$|\pmb{S}| = `r det(S)`$.

b.) Find the total sample variance $tr(S)$ as in (3.78).

```{r}
tr_S <- sum(diag(S))
```

$tr(S) = `r tr_S`$.

### Problem 12

```{r}
y1 <- c(51,27,37,42,27,43,41,38,36,26,29)
y2 <- c(36,20,22,36,18,32,22,21,23,31,20)
y3 <- c(50,26,41,32,33,43,36,31,27,31,25)
y4 <- c(35,17,37,34,14,35,25,20,25,32,26)
y5 <- c(42,27,30,27,29,40,38,16,28,36,25)

Y <- cbind(y1,y2,y3,y4,y5) |> unname()
n <- nrow(Y)
I <- diag(ncol=n, nrow=n)
J <- matrix(rep(1, times=n^2), ncol=n)
S <- t(Y)%*%(I - J/n)%*%Y/(n-1)
```

a.) Find the generalised sample variance of the probe word data $|\pmb{S}|$ as
in (3.77).

```{r}
#compare this result to spectral decomposition
S_eigs <- eigen(S, only.values=TRUE)
all.equal(det(S), prod(S_eigs$values))
```

$|\pmb{S}| = `r det(S)`$.

b.) Find the total sample variance $tr(S)$ as in (3.78).

```{r}
tot_var <- sum(diag(S))
```

$tr(S) = `r tot_var`$.

### Problem 13

```{r}
d <- 1/sqrt(diag(S))
D_inv <- diag(d, ncol=length(d), nrow=length(d))
R <- D_inv%*%S%*%D_inv |> round(3)
ltx_R <- latexMatrix(R, matrix="pmatrix")$matrix
```

$\pmb{R} = `r ltx_R`$.

### Problem 14

```{r}
y1 <- c(35,35,40,10,6,20,35,35,35,30)
y2 <- c(3.5,4.9,30.0,2.8,2.7,2.8,4.6,10.9,8.0,1.6)
y3 <- c(2.80,2.70,4.38,3.21,2.73,2.81,2.88,2.90,3.28,3.20)

Y <- cbind(y1,y2,y3) |> unname()
a <- matrix(c(3,-1,2), ncol=1)
```
a.) Evaluate $z$ for each row of the calcium data and find $\bar{z}$ and $s^2_z$
directly from $z_1, z_2, ..., z_{10}$ using (3.1) and (3.5).

```{r}
z <- Y%*%a
n <- length(z)

z_bar <- sum(z)/n
s_z <- 0
for(z_i in z) {
  s_z <- s_z + (z_i - z_bar)^2
}
s_z <- s_z/(n-1)
```
$\bar{z} = `r z_bar`$, $s^2_z = `r s_z`$. 


b.) Use $\bar{z} = \pmb{a}'\pmb{\bar{y}}$ and $s_z = \pmb{a}'\pmb{S}\pmb{a}$ as
in (3.54) and (3.55).

```{r}
j <- matrix(rep(1, times=n), ncol=1)
y_bar <- t(Y)%*%j/n
z_bar <- t(a)%*%y_bar
```

$\bar{z} = `r z_bar`$

```{r}
I <- diag(ncol=n, nrow=n)
J <- matrix(rep(1, times=n^2), ncol=n)
S <- t(Y)%*%(I - J/n)%*%Y/(n-1)
s_z <- t(a)%*%S%*%a
```
$s^2_z = `r s_z`$.


```{r}
all.equal(s_z, var(z))
```

### Problem 15

a.) Evaluate $z$ and $w$ for each row of the calcium data and fine $r_{wz}$ from
the 10 pairs $(z_i,w_i)$, $i=1,2,...,10$, using (3.10) and (3.13).
```{r}
b <- matrix(c(-2,3,1), ncol=1)
w <- Y%*%b
w_bar <- t(b)%*%y_bar

s_wz <- s2_z <- s2_w <-0
for(i in 1:10) {
  s_wz <- s_wz + (z[i]*w[i])
  s2_z <- s2_z + (z[i] - z_bar)^2
  s2_w <- s2_w + (w[i] - w_bar)^2
}

s_wz <- s_wz - n*z_bar*w_bar
r_wz <- s_wz/sqrt(s2_z*s2_w)
```

$$
r_{wz} = \frac{\sum_{i=1}^n(z_i - \bar{z})(w_i - \bar{w})}{\sqrt{\sum_{i=1}^n(z_i - \bar{z})^2\sum_{i=1}^n(w_i - \bar{w})^2}} = `r r_wz`
$$
b.) Find $r_{wz}$ using (3.57).

```{r}
s_wz <- t(a)%*%S%*%b
r_wz <- s_wz/sqrt(t(a)%*%S%*%a * t(b)%*%S%*%b)
```

$$
r_{wz} = \frac{\pmb{a}'\pmb{S}\pmb{b}}{\sqrt{\pmb{a}'\pmb{S}\pmb{a}\cdot\pmb{b}'\pmb{S}\pmb{b}}} = `r r_wz`
$$

### Problem 16
Find the correlation between $y_1$ and $\frac{1}{2}(y_2+y_3)$ using (3.57).

```{r}
a <- matrix(c(1,0,0), ncol=1)
b <- matrix(c(0,1/2,1/2), ncol=1)
z <- Y%*%a
w <- Y%*%b

r_wz <- t(a)%*%S%*%b/sqrt(t(a)%*%S%*%a * t(b)%*%S%*%b)
ltx_a <- latexMatrix(a, matrix="pmatrix")$matrix
ltx_b <- latexMatrix(b, matrix="pmatrix")$matrix
```
Here, define $\pmb{z} = \pmb{Ya}$ and $\pmb{w} = \pmb{Yb}$ where
$\pmb{a} = `r ltx_a`$ and $\pmb{b} = `r ltx_b`$.

Then,

$$
r_{wz} = \frac{\pmb{a}'\pmb{S}\pmb{b}}{\sqrt{\pmb{a}'\pmb{S}\pmb{a}\cdot\pmb{b}'\pmb{S}\pmb{b}}} = `r r_wz`.
$$
A quick check with `all.equal(r_wz, cor(w,z))` confirms that this is correct.

### Problem 17

Find $\pmb{\bar{z}}$ and $\pmb{S}_z$ using (3.62) and (3.64).

```{r}
A <- matrix(c(1,1,1,
              2,-3,2,
              -1,-2,-3), ncol=3, byrow=TRUE)

z_bar <- A%*%y_bar
S_z <- A%*%S%*%t(A)
ltx_z_bar <- latexMatrix(z_bar, matrix="pmatrix")$matrix
ltx_S_z <- latexMatrix(S_z, matrix="pmatrix")$matrix
```
$\pmb{\bar{z}} = `r ltx_z_bar`$.

$\pmb{S}_z = `r ltx_S_z`$.

b.) Find $\pmb{R}_z$ from $\pmb{S}_z$ using (3.37).

```{r}
d <- 1/sqrt(diag(S_z))
D_inv <- diag(d, ncol=length(d), nrow=length(d))
R_z <- D_inv%*%S_z%*%D_inv
ltx_R_z <- latexMatrix(R_z, matrix="pmatrix")$matrix
```


$$
\pmb{R} = \pmb{D}^{-1}\pmb{S}\pmb{D}^{-1}\tag{3.37}
$$

$\pmb{R}_z = `r ltx_R_z`$. 


### Problem 18

```{r}
y1 <- c(47.8,46.4,46.3,45.1,47.6,52.5,51.2,49.8,48.1,45.0,51.2,48.5,52.1,48.2,49.6,50.7,47.2,53.3,46.2,46.3)
y2 <- c(48.8,47.3,46.8,45.3,48.5,53.2,53.0,50.0,50.8,47.0,51.4,49.2,52.8,48.9,50.4,51.7,47.7,54.6,47.5,47.6)
y3 <- c(49.0,47.7,47.8,46.1,48.9,53.3,54.3,50.3,52.3,47.3,51.6,53.0,53.7,49.3,51.2,52.7,48.4,55.1,48.1,51.3)
y4 <- c(49.7,48.4,48.5,47.2,49.3,53.7,54.4,52.7,54.4,48.3,51.9,55.5,55.0,49.8,51.8,53.3,49.5,55.3,48.4,51.8)

Y <- cbind(y1,y2,y3,y4) |> unname()
```

a.) Find $\pmb{\bar{y}}$, $\pmb{S}$, and $\pmb{R}$

```{r}
n <- nrow(Y)
j <- matrix(rep(1, times=n), ncol=1)
# create a column vector of the variable means
y_bar <- t(Y)%*%j/n

I <- diag(ncol=n, nrow=n)
J <- matrix(rep(1, times=n^2), ncol=n)
S <- t(Y)%*%(I - J/n)%*%Y/(n-1)
d <- 1/sqrt(diag(S))
D_inv <- diag(d, ncol=length(d), nrow=length(d))
R <- D_inv%*%S%*%D_inv
ltx_ybar <- latexMatrix(y_bar, matrix="pmatrix")$matrix
ltx_S <- latexMatrix(S, matrix="pmatrix")$matrix
ltx_R <- latexMatrix(R, matrix="pmatrix")$matrix
```

$\pmb{\bar{y}} = `r ltx_ybar`$.

$\pmb{S}= `r ltx_S`$.

$\pmb{R} = `r ltx_R`$.

b.) Find $|\pmb{S}|$ and $tr(\pmb{S})$.

```{r}
tr_S <- sum(diag(S))
```

$|\pmb{S}| = `r det(S)`$.

$tr(\pmb{S}) = `r tr_S`$.

### Problem 19

a.) Find $\bar{z}$, $\bar{w}$, $s^2_z$, and $s^2_w$ using (3.54) and (3.55).

```{r}
a <- matrix(c(1,2,1,-3), ncol=1)
b <- matrix(c(-2,3,-1,2), ncol=1)

n <- nrow(Y)
j <- matrix(rep(1, times=n), ncol=1)
y_bar <- t(Y)%*%j/n

z_bar <- t(a)%*%y_bar
w_bar <- t(b)%*%y_bar

I <- diag(ncol=n, nrow=n)
J <- matrix(rep(1, times=n^2), ncol=n)

S <- t(Y)%*%(I - J/n)%*%Y/(n-1)

s2_z <- t(a)%*%S%*%a
s2_w <- t(b)%*%S%*%b
```

$\bar{z} = `r z_bar`$, $\bar{w} = `r w_bar`$, $s^2_z = `r s2_z`$, 
$s^2_w = `r s2_w`$.

b.) Find $s_{wz}$ and $r_{wz}$ using (3.56) and (3.57).

```{r}
s_wz <- t(a)%*%S%*%b
r_wz <- s_wz/sqrt(s2_z*s2_w)
```
$s_{wz} = `r s_wz`$, $r_{wz} = `r r_wz`$.

### Problem 20
Find $\pmb{\bar{z}}$, $\pmb{S}_z$, and $\pmb{R}_z$ using (3.62), (3.64), and
(3.37) respectively.

```{r}
A <- matrix(c(2,3,-1,4,
              -2,-1,4,-2,
              3,-2,-1,3), ncol=4, byrow=TRUE)
n <- nrow(Y)
j <- matrix(rep(1, times=n), ncol=1)
y_bar <- t(Y)%*%j/n 

z_bar <- A%*%y_bar
S_z <- A%*%S%*%t(A)

d <- 1/sqrt(diag(S_z))
D_inv <- diag(d, ncol=length(d), nrow=length(d))
R_z <- D_inv%*%S_z%*%D_inv
ltx_z_bar <- latexMatrix(z_bar, matrix="pmatrix")$matrix
ltx_S_z <- latexMatrix(S_z, matrix="pmatrix")$matrix
ltx_R_z <- latexMatrix(R_z, matrix="pmatrix")$matrix
```
$\pmb{\bar{z}} = `r ltx_z_bar`$.

$\pmb{S}_z = `r ltx_S_z`$.

$\pmb{R}_z = `r ltx_R_z`$.





