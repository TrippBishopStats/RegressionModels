---
title: "Principal Components Analysis"
format: html
execute:
  warning: false
  messages: false
---

```{r}
#| echo: false
library(psych)
library(matlib)
library(kableExtra)
library(tidyverse)

theme_set(theme_minimal())
```

## Chapter summary
Principal component analysis is primarily a dimension reduction technique. It
uses axes rotation to align the coordinate system along the paths of maximum
variance, creating a new set of uncorrelated (orthogonal) components. There are
no predictors or response variables in PCA as it is not a model. True 
multicolinearity does not affect the technique as it does not make any 
assumptions about the nature of the data. Component interpretations can, 
however, be difficult due to the blending of variables into the new coordinate
axes. PCA is not useful if the original variables are independent of each other
as in this case they are already the principal components.

## Main points
1) The spectral decomposition of the covariance matrix $\pmb{S}$ provides the
rotation to align the coordinate system to maximal variance. The eigenvectors
of a positive definite matrix are orthogonal.
2) It is necessary to scale variables in the situation where the scales of the
variables are very different. If a few variables are on much larger scales and
with correspondingly larger variances, they will dominate the first few 
components.
3) Either the covariance matrix or the correlation matrix of variables can be
used. The spectral decompositions will be different. $\pmb{R}$ is scale
invariant, so all differences are in terms of standard deviations not the
original units of the variables.
4) The eigenvalues of the spectral decomposition represent the variance along
each principal component. The sum of the eigenvalues equals the trace of 
$\pmb{S}$.
5) There are 4 main techniques for determining how many principal components to
keep for further analysis: cumulative variance percentage, mean eigenvalue 
cutoff, scree plots, and hypothesis testing.

### Example
Given the following covariance matrix, we can use the `eigen` function to 
perform a spectral decomposition to obtain eigenvalues and eigenvectors of 
$\pmb{S}$.

```{r}
S <- matrix(
  c(0.370,0.602,0.149,0.044,0.107,0.209,
    0.602,2.629,0.801,0.666,0.103,0.377,
    0.149,0.801,0.458,0.011,-0.013,0.120,
    0.044,0.666,0.011,1.474,0.252,-0.054,
    0.107,0.103,-0.013,0.252,0.488,-0.036,
    0.209,0.377,0.120,-0.054,-0.036,0.324),
  byrow=TRUE,
  ncol=6,
  nrow=6
)
ltx_S <- latexMatrix(S, matrix="pmatrix")$matrix
eigs <- eigen(S, symmetric=TRUE)
total_var <- sum(eigs$values)
```

$\pmb{S} = `r ltx_S`$

We can confirm that the sum of the eigen values equals the sum of the $s_{jj}$
elements of $\pmb{S}$.

$\sum_{j=1}^6 = `r sum(diag(S))` = `r total_var` = \sum_{i=1}^6 \lambda_i$.

So that checks out. Now, we can determine what percentage of the total variance
each principal component accounts for:

```{r}
pct_var <- eigs$values/total_var
cum_pct <- cumsum(pct_var)
mtx_var <- rbind(pct_var,cum_pct)
rownames(mtx_var) <- c("Variance", "Cumulative Variance")
kable(mtx_var,
      col.names= paste("PC", 1:6),
      digits=3,
      caption="Percentage of variance accounted for by each principal component.")
```

From this, we can see that the first two principal components account for a 
little more than 81% of the total variability in the data.



```{r}
# find out why eigen produces exactly opposite eigen vectors to what is reported
# by Rencher.
A <- -1*t(eigs$vectors)[1:2,] |> round(3) 
ltx_A <- latexMatrix(A, matrix="pmatrix")$matrix
```

If we only wanted to retain the first two principal components, then the matrix
of component scores would be
$\pmb{A} = `r ltx_A`$
From this matrix, we can see that the largest component scores are associated 
with the variables $y_2$ and $y_4$. From the $\pmb{S}$ matrix, we can see that
these two variables have the largest variances, and so it makes sense that they
would dominate the largest principal components.

::: {.callout-note}
## Scale matters
If the scales of the variables in the dataset are quite different, it will be
necessary to standardise the data before performing the rotation. if the scales
are very different, then the variables with the largest raw scores are nearly
assured to dominate the analysis.
:::

## Retaining components

We already saw how we can use the percentage of variance explained to make a
decision about which components to retain. We can also compare eigenvalues to 
the average eigenvalue, keeping those that are above average.

```{r}
avg_eigval <- mean(eigs$values)
kable(t(eigs$values),
      col.names= paste("PC", 1:6),
      digits=3,
      caption="Eigenvalues associated with each principal component.")
```

The mean eigenvalue is `r round(avg_eigval, 3)`, so all eigenvalues of $\pmb{S}$
greater than this value would typically be retained. **Note**: if using a 
correlation matrix instead of a covariance matrix the mean eigenvalue will be
one.

```{r}
R <- cov2cor(S)
eigs_R <- eigen(R)
lambda_bar_R <- mean(eigs_R$values)
```
$\bar{\lambda}_R = `r lambda_bar_R`$.

We can also make a judgement graphically through the use of a scree plot. The
idea here is that we look for the point where the plot transitions from a steep
angle of decent to a relatively gentle slope. The eigenvalue to the left of the
inflection point indicates which principal component is the last to be retained.
In the plot below that occurs between the second and third eigenvalue so only 
the first two principal components should be retained. This is consistent with 
the previous two methods what have been applied.

```{r}
#| fig-width: 6
#| fig-height: 4
#| fig-align: "center"

data.frame(
  eignum = 1:length(eigs$values),
  eigval = eigs$values
) |> ggplot(aes(x=eignum, y=eigval)) +
  geom_point(colour="steelblue") + geom_line(colour="steelblue") +
  labs(
    title="Scree plot for component selection",
    x="Eigen number",
    y="Eigenvalue"
  )
```

Hypothesis testing is the final method for determining which component(s) to
retain for analysis. This method uses an iterative process to create a $\chi^2$
test statistic to test the null hypothesis that the $k$ smallest population
eigenvalues, $\gamma_{p-k},...,\gamma_p$ are equal. The test statistic, $u$ has
the following form
$$
u = \bigg(n - \frac{2p+11}{6}\bigg)\bigg(k\ln \bar{\lambda} - \sum_{i=p-k+1}^p\ln\lambda_i\bigg)
$$
and is approximately $\chi^2$ distributed. $H_0$ is rejected if 
$u\geq\chi^2_{\alpha,\nu}$ where $\nu = \frac{1}{2}(k-1)(k+2)$.

:::{.callout-warning}
Why does this work and how is it a likelihood ratio test?
:::

## Exercises

### Problem 7
First, set up the data frame and use it to create the covariance and correlation
matrices that will be used to conduct the PCA.

```{r}
# probe word data
y1 <- c(51,27,37,42,27,43,41,38,36,26,29)
y2 <- c(36,20,22,36,18,32,22,21,23,31,20)
y3 <- c(50,26,41,32,33,43,36,31,27,31,25)
y4 <- c(35,17,37,34,14,35,25,20,25,32,26)
y5 <- c(42,27,30,27,29,40,38,16,28,36,25)

Y <- cbind(y1,y2,y3,y4,y5) |> unname()
n <- nrow(Y)
I <- diag(nrow=n, ncol=n)
J <- matrix(rep(1, times=n*n), ncol=n, nrow=n)
# now create the covariance matrix S
S <- t(Y)%*%(I - 1/n*J)%*%Y/(n-1)
p <- ncol(S)
D <- sqrt(diag(diag(S), p, p)) |> solve()
R <- D%*%S%*%D

ltx_S <- latexMatrix(S, matrix="pmatrix")$matrix
ltx_R <- latexMatrix(R, matrix="pmatrix")$matrix
```

Let's start by using $\pmb{S}$ to carry out the PCA. Let's examine $\pmb{S}$
first to see if the variances are on very different scales.

$\pmb{S} = `r ltx_S`$

The variances are all pretty similar, so using the covariance matrix for this
analysis should be okay.

```{r}
eigs_S <- eigen(S)
lambda_bar <- mean(eigs_S$values)
total_var <- sum(eigs_S$values)
pct_var <- eigs_S$values/total_var
mtx_S_eigs <- cbind(pct_var, cumsum(pct_var))
kable(mtx_S_eigs, digits=3, col.names=c("Pct", "Cum pct"))
```

```{r}
eigs_R <- eigen(R)
lambda_bar <- mean(eigs_R$values)
total_var <- sum(eigs_R$values)
pct_var <- eigs_R$values/total_var
mtx_R_eigs <- cbind(pct_var, cumsum(pct_var))
kable(mtx_R_eigs, digits=3, col.names=c("Pct", "Cum pct"))
```

The variance explained by the components of from $\pmb{S}$ and $\pmb{R}$ are
nearly identical, so either one is a reasonable choice for the analysis.

```{r}
tibble(
  eignum = 1:length(eigs_S$values),
  eigvals = eigs_S$values
) |> ggplot(aes(x=eignum, y=eigvals)) + geom_point() + geom_line()
```

```{r}
eigs_S$vectors
```

```{r}
eigs_R$vectors
```
The eigenvectors of $\pmb{S}$ and $\pmb{R}$ are similar. There isn't an obvious
interpretation of either set of component scores.

### Problem 10

```{r}
males <- matrix(
  c(15,17,24,14,
    17,15,32,26,
    15,14,29,23,
    13,12,10,16,
    20,17,26,28,
    15,21,26,22,
    15,13,26,22,
    13,5,22,22,
    14,7,30,17,
    17,15,30,27,
    17,17,26,20,
    17,20,28,24,
    15,15,29,24,
    18,19,32,28,
    18,18,31,27,
    15,14,26,21,
    18,17,33,26,
    10,14,19,17,
    18,21,30,29,
    18,21,34,26,
    13,17,30,24,
    16,16,16,16,
    11,15,25,23,
    18,18,34,34,
    16,15,28,27,
    15,16,29,24,
    18,19,32,33,
    18,16,33,23,
    17,20,21,21,
    19,19,30,28),
  ncol=4,
  byrow=TRUE
)

females <- matrix(
  c(13,14,12,21,
    14,12,14,26,
    12,19,21,21,
    12,13,10,16,
    11,20,16,16,
    12,9,14,18,
    10,13,18,24,
    10,8,13,23,
    12,20,19,23,
    11,10,11,27,
    12,18,25,25,
    14,18,13,26,
    14,10,25,28,
    13,16,8,14,
    14,8,13,25,
    13,16,23,28,
    16,21,26,26,
    14,17,14,14,
    16,16,15,23,
    13,16,23,24,
    2,6,16,21,
    14,16,22,26,
    17,17,22,28,
    16,13,16,14,
    15,14,20,26,
    12,10,12,9,
    14,17,24,23,
    13,15,18,20,
    11,16,18,28,
    7,7,19,18,
    12,15,7,28,
    6,5,6,13),
  ncol=4,
  byrow=TRUE
)
```

#### Males

First, create the covariance matrix from the data and perform a spectral 
decomposition to obtain the eigenvalues and eigenvectors.

```{r}
n <- nrow(males)
I <- diag(nrow=n, ncol=n)
J <- matrix(rep(1,times=n*n), ncol=n, nrow=n)
S <- t(males)%*%(I - 1/n*J)%*%males/(n-1)
ltx_S <- latexMatrix(S, matrix="pmatrix")$matrix
```

$\pmb{S}=`r ltx_S`$

Now, perform the spectral decomposition and show the percentage of variance 
accounted for by each principal component and the cumulative percentages.

```{r}
# symmetric=FALSE makes the eigen values consistent w/prcomp
eigs_S <- eigen(S, symmetric=FALSE)
pct_vals <- eigs_S$values/matlib::tr(S)
mtx_eigs <- rbind(pct_vals, cumsum=cumsum(pct_vals))
kable(mtx_eigs, digits=3, col.names=paste("PC", 1:4))
```

The table above gives us a good indication of how much variance is accounted for
by each principal component. We can use the mean eigenvalue and a scree plot to
gather evidence for how many principal components to retain.

```{r}
lambda_bar <- mean(eigs_S$values)
tibble(
  eignum=1:4,
  eigval=eigs_S$values
) |> ggplot(aes(x=eignum, y=eigval)) + geom_point() + geom_line() + 
  geom_hline(yintercept=lambda_bar, linetype="dashed")
```

These methods suggest that only the first component should be retained, whereas
the 80% threshold indicates that the first two principal components should be 
kept.

```{r}
eigs_S$vectors[,1:2]
```
$y_3$ and $y_4$ look to load primarily on `PC1` and $y_2$ and $y_3$ on `PC2`. It
isn't too surprising that $y_1$ has low component scores across both components
because it has the smallest variance. Interestingly, it is well represented by 
`PC4`.

#### Rotating a plotting the data

```{r}
A <- t(eigs_S$vectors)

PCA <- males%*%A

tibble(
  x=PCA[,1],
  y=PCA[,2]
) |> ggplot(aes(x,y)) + geom_point()
```

